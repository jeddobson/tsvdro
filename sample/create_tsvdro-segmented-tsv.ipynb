{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create TSV-DRO: Segmented Windows of Tokens </h1>\n",
    "\n",
    "A simple Notebook to encapsulate a text file as TSV-DRO. Define the size of segments and this will segment text into token counts per segment. \n",
    "\n",
    "James E. Dobson (james.e.dobson@dartmouth.edu)<br>\n",
    "Dartmouth College<br>\n",
    "http://www.dartmouth.edu/~jed <br>\n",
    "\n",
    "08/24/2018: Initial Version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load required modules\n",
    "import tsvdro\n",
    "import csv\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text_object, options = \"default\"):\n",
    "        from nltk.corpus import stopwords\n",
    "        stopwords = stopwords.words('english')\n",
    "\n",
    "        # *step one* (default): drop to lowercase\n",
    "        pp_text = [word.lower() for word in text_object]\n",
    "\n",
    "        # *step two* (default): remove non-alpha characters,\n",
    "        # punctuation, and as many other \"noise\" elements as\n",
    "        # possible. If dealing with a single character word,\n",
    "        # drop non-alphabetical characters. This will remove\n",
    "        # most punctuation but preserve many words containing\n",
    "        # marks such as the '-' in 'self-emancipated'\n",
    "\n",
    "        tmp_text=list()\n",
    "        for word in pp_text:\n",
    "                if len(word) == 1:\n",
    "                        if word.isalpha == True:\n",
    "                                tmp_text.append(word)\n",
    "                else:\n",
    "                        tmp_text.append(word)\n",
    "\n",
    "        pp_text = tmp_text\n",
    "        tmp_text=list()\n",
    "\n",
    "        # now remove leading and trailing quotation marks,\n",
    "        # hyphens and  dashes\n",
    "\n",
    "        drop_list = [u'“'.encode('utf-8'),'\"',u'”'.encode('utf-8'),'-','—']\n",
    "        for word in pp_text:\n",
    "                if word[0].encode('utf-8') in drop_list:\n",
    "                        word = word[1:]\n",
    "                if word[-1:].encode('utf-8') in drop_list:\n",
    "                        word = word[:-1]\n",
    "                # catch any zero-length words remaining\n",
    "                if len(word) > 0:\n",
    "                        tmp_text.append(word)\n",
    "\n",
    "        pp_text = tmp_text\n",
    "\n",
    "        # preprocessing function: preserve *ONLY* NLTK stopwords\n",
    "        if options == \"onlystop\":\n",
    "                pp_text = [word for word in pp_text if word in stopwords]\n",
    "                return(pp_text)\n",
    "\n",
    "        # *step three* (default): remove stopwords\n",
    "        # enable an option for preserving stopwords\n",
    "        if options != \"nostop\":\n",
    "                # add additional stopwords, also containing some remainders from\n",
    "                # tokenizing\n",
    "                custom_stopwords=\"\"\"like go going gone one said says would got still really get 's 'll n't\"\"\"\n",
    "                stopwords += custom_stopwords.split()\n",
    "                \n",
    "                pp_text = [word for word in pp_text if word not in stopwords]\n",
    "\n",
    "        return(pp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Describe this object to the best of your ability\n",
    "\n",
    "file_name = 'New_England_Girlhood.txt'\n",
    "author_name = 'Lucy Larcom'\n",
    "title = 'A New England Girlhood'\n",
    "publisher = ''\n",
    "publisher_location = ''\n",
    "publication_date = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsvdro_object = dict()\n",
    "tsvdro_object['header'] = tsvdro.build_header()\n",
    "tsvdro_object['header']['bibliographic_data']['file_uri'] = ''\n",
    "tsvdro_object['header']['bibliographic_data']['author_name'] = author_name\n",
    "tsvdro_object['header']['bibliographic_data']['title'] = title\n",
    "\n",
    "# produce TSV token count\n",
    "raw_text = open(file_name,encoding=\"utf-8\").read()\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "# update raw token count\n",
    "tsvdro_object['header']['workflow']['token_count']  = len(text)\n",
    "text = preprocess(text)\n",
    "\n",
    "# update vocab count\n",
    "tsvdro_object['header']['workflow']['vocab_count'] = len(set(text))\n",
    "\n",
    "# set type to segmented tsv (data_type: 2)\n",
    "# how many words make up a segment?\n",
    "\n",
    "tsvdro_object['header']['workflow']['data_type'] = 2\n",
    "tsvdro_object['header']['workflow']['data_option'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create segments \n",
    "segments=0\n",
    "\n",
    "# now build TSV\n",
    "tsvdro_object['data'] = list()\n",
    "\n",
    "# use preprocessed word count\n",
    "if len(text) % 1000 != 0:\n",
    "    segments = int(len(text) / 1000) + 1\n",
    "else:\n",
    "    segments = len(text) / 1000\n",
    "\n",
    "start = 0 \n",
    "segment_data = dict()\n",
    "for seg in range(segments):\n",
    "    for token in set(text[start:start+1000]):\n",
    "        segment_data[token] = text[start:start+1000].count(token)\n",
    "    tsvdro_object['data'].append(segment_data)\n",
    "    start += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = file_name.replace(\".txt\",\".dro\")\n",
    "tsvdro.save(tsvdro_object,output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
